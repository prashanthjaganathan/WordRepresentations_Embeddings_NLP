{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Evaluation: Analogy Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "In this notebook, we evaluate word embeddings by solving an analogy task. The goal is to find a word `d` that satisfies the analogy \"a : b = c : d,\" using the vector representations (embeddings) of the words.\n",
    "\n",
    "## Methodology\n",
    "We will follow the approach proposed by [Mikolov et al. (2013)](https://arxiv.org/pdf/1301.3781), where we use simple vector algebra to compute word analogies. Specifically, given the embeddings for three words `(a, b, c)`, we seek the word `d` that maximizes the following expression from the pre-trained embedding:\n",
    "\n",
    "$$\n",
    "d = \\arg \\max_{w \\in V \\setminus \\{b, a, c\\}} \\cos\\left(\\mathbf{v}_w, \\mathbf{v}_a - \\mathbf{v}_b + \\mathbf{v}_c \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- â€‹$\\mathbf{v}_a, \\mathbf{v}_b, \\mathbf{v}_c, \\mathbf{v}_d$, are the embeddings (vector representations) of the words `a`, `b`, `c`, and `d`.\n",
    "- The set  `V` represents the vocabulary, and we exclude the words \\( a, b, \\) and \\( c \\) from consideration.\n",
    "- The cosine similarity $\\cos(\\cdot)$ is used to measure the closeness between word vectors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We aim to find the word `d` whose vector embedding satisfies this relationship as closely as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import re\n",
    "import random\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be using pre-trained embeddings for our analogy task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pretrained Word2Vec \n",
    "\n",
    "Google published pre-trained vectors trained on part of Google News dataset (about 6 billion words in the corpus). For this project, we will be using the 50 dimensional embedding model for evaluation. \n",
    "\n",
    "Model Configuration: \n",
    "- 6B corpus\n",
    "- 3M words vocab \n",
    "- 300 dims vector\n",
    "- 10 ~ context window\n",
    "- used negative sampling for training\n",
    "\n",
    "Article can be found [here](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "Model can be downloaded [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Path to the binary Word2Vec model\n",
    "current_directory = os.getcwd()\n",
    "dataset_folder = os.path.join(current_directory, 'dataset/')\n",
    "model_path = dataset_folder + 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Load the model\n",
    "word2vec_word_embeddings = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "words = word2vec_word_embeddings.index_to_key\n",
    "word2vec_word_embeddings_dict = {}\n",
    "for word in words:\n",
    "    word2vec_word_embeddings_dict[word] = word2vec_word_embeddings[word]\n",
    "print(\"Embeddings loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pretrained GloVe Model: \n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "Model Configuration: \n",
    "- Trained on Wikipedia 2014 + Gigaword 5\n",
    "- 6B tokens\n",
    "- 400K vocab\n",
    "- uncased\n",
    "- 50d, 100d, 200d, & 300d vectors\n",
    "\n",
    "Article link and model can be downloaded [here](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_glove_embeddings(path: str) -> Dict[str, List[float]]:\n",
    "    \"\"\"Preprocess .txt file into a Dict with word as key and value is embedding vector\"\"\"\n",
    "    word_embeddings_dict: Dict[str, List[float]] = {}\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            individual_dims = line.strip().split()\n",
    "            word = individual_dims[0]\n",
    "            word_embedding = np.array(individual_dims[1:], dtype=np.float32)\n",
    "            word_embeddings_dict[word] = word_embedding\n",
    "\n",
    "    return word_embeddings_dict\n",
    "\n",
    "# GloVe requires some preprocessing as the embeddings come in a .txt file\n",
    "model_path = dataset_folder + 'glove/glove.6B.50d.txt'\n",
    "glove_word_embeddings_dict = get_glove_embeddings(model_path)\n",
    "print(\"Embeddings loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be working only on a subset (few groups) of Mikolov's Analogy Dataset.\n",
    "\n",
    "Dataset can be found [here](https://www.fit.vut.cz/person/imikolov/public/rnnlm/word-test.v1.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preprocessed successfully!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(path: str):\n",
    "    processed_dataset_dict: Dict[str, List[Tuple]] = {}\n",
    "    # Goal is to pre-process into a dict of valid groups {'group name': [(a, b, c, d), ....]}\n",
    "    valid_groups = ['capital-world', 'currency', 'city-in-state', \n",
    "                    'family', 'gram1-adjective-to-adverb', 'gram2-opposite', \n",
    "                    'gram3-comparative', 'gram6-nationality-adjective']\n",
    "    \n",
    "    group_name = ''\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('//'): \n",
    "                continue # Ignore the first line\n",
    "\n",
    "            if line.startswith(': '): # check if line begins with ':'\n",
    "                match = re.match(r':\\s*(\\S+)', line)  # Match a group name after ': ' using RegEx\n",
    "                group_name = match.group(1)\n",
    "                if group_name in valid_groups:\n",
    "                    processed_dataset_dict[group_name] = []\n",
    "                    continue\n",
    "                else: \n",
    "                    group_name = ''\n",
    "\n",
    "            if group_name in processed_dataset_dict:\n",
    "                words = line.split()\n",
    "\n",
    "                # Arrange to expected format (a, b, c, d)\n",
    "                words = (words[0], words[1], words[2], words[3])\n",
    "                processed_dataset_dict[group_name].append(words) \n",
    "            \n",
    "    return processed_dataset_dict\n",
    "\n",
    "dataset_path = dataset_folder + 'word-test.v1.txt'\n",
    "dataset = preprocess_dataset(dataset_path)\n",
    "print('Dataset preprocessed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's write the methods that will help us measure the cosine similarity and the accuracy of the pretrained embeddings on Mikolov's Analogy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_analogy(a, b, c, embeddings):\n",
    "    \"\"\"Computes the best fit word `d` in (a, b, c, d) using cosine similarity\"\"\"\n",
    "    if a not in embeddings or b not in embeddings or c not in embeddings:\n",
    "        return None\n",
    "    \n",
    "    a_vec = embeddings[a]\n",
    "    b_vec = embeddings[b]\n",
    "    c_vec = embeddings[c]\n",
    "    \n",
    "    result_vec = b_vec - a_vec + c_vec\n",
    "    \n",
    "    # Find the most similar word, ie. word with the least cosine distance\n",
    "    best_similarity = float('inf')\n",
    "    best_word = None\n",
    "    \n",
    "    for word, vec in embeddings.items():\n",
    "        if word in {a, b, c}: \n",
    "            continue\n",
    "\n",
    "        similarity = cosine(result_vec, vec)\n",
    "        if similarity < best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_word = word\n",
    "    \n",
    "    return best_word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy(embeddings, dataset, rand = True):\n",
    "    \"\"\"Computes the accuracy of the embeddings passed w.r.t the dataset questions\n",
    "    Also, prints the accuracy obtained on individual groups in the dataset.\n",
    "    \"\"\"\n",
    "    total_correct = 0\n",
    "    cummulative_total = 0\n",
    "    for group, quads in dataset.items():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for (a, b, c, d) in random.sample(quads, 25) if rand else quads:\n",
    "            word_pred = compute_analogy(a.lower(), b.lower(), c.lower(), embeddings)\n",
    "            \n",
    "            if word_pred == d.lower():\n",
    "                correct += 1\n",
    "                total_correct += 1\n",
    "            \n",
    "            total += 1\n",
    "            cummulative_total += 1\n",
    "\n",
    "        # Accuracy is calculated by comparing `d` with the most similar word in the embedding space,\n",
    "        # ie. performing similarity check or every single embedding available in the pre-trained embeddings.\n",
    "        print(f'Accuracy on group {group}: {correct / total}')\n",
    "\n",
    "    return total_correct / cummulative_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "Since, calculating the accuracy of every analogy question for every group in the dataset is very very computational expensive (~5000 questions for each group), I'll be picking 25 random analogy questions from every group. \n",
    "\n",
    "PS: this required an hour of computation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing accuracies on both sets of pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Evaluating the Word2Vec Model --------------\n",
      "\n",
      "Accuracy on group capital-world: 0.04\n",
      "Accuracy on group currency: 0.08\n",
      "Accuracy on group city-in-state: 0.04\n",
      "Accuracy on group family: 0.88\n",
      "Accuracy on group gram1-adjective-to-adverb: 0.24\n",
      "Accuracy on group gram2-opposite: 0.2\n",
      "Accuracy on group gram3-comparative: 0.96\n",
      "Accuracy on group gram6-nationality-adjective: 0.28\n",
      "Overall accuracy across all groups: 0.34\n",
      "\n",
      "\n",
      "-------------- Evaluating the GloVe Model ----------------\n",
      "Accuracy on group capital-world: 0.68\n",
      "Accuracy on group currency: 0.12\n",
      "Accuracy on group city-in-state: 0.16\n",
      "Accuracy on group family: 0.68\n",
      "Accuracy on group gram1-adjective-to-adverb: 0.0\n",
      "Accuracy on group gram2-opposite: 0.12\n",
      "Accuracy on group gram3-comparative: 0.44\n",
      "Accuracy on group gram6-nationality-adjective: 0.84\n",
      "Overall accuracy across all groups: 0.38\n"
     ]
    }
   ],
   "source": [
    "print('------------- Evaluating the Word2Vec Model --------------\\n')\n",
    "word2vec_acc = measure_accuracy(word2vec_word_embeddings_dict, dataset)\n",
    "print(f'Overall accuracy across all groups: {word2vec_acc}\\n\\n')\n",
    "\n",
    "\n",
    "print('-------------- Evaluating the GloVe Model ----------------')\n",
    "glove_acc = measure_accuracy(glove_word_embeddings_dict, dataset)\n",
    "print(f'Overall accuracy across all groups: {glove_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "Since, we are selecting 25 random questions from every group and comparing, we can't really measure which set of embeddings are better for the given dataset. Measuring the accuracy on the entire dataset, will give us a better idea of the performance of the pre-trained embeddings in our use case.\n",
    "\n",
    "But, overall we can see that on few groups both the pre-trained sets is able to generalize well on the dataset. However, this highlights the importance of training embeddings for every use case would provide more quality in the output, however, at the cost of computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Antonyms\n",
    "\n",
    "A major problem with word embeddings is that antonyms (words with meanings considered to be opposites) often have SIMILAR embeddings. \n",
    "\n",
    "Let's verify this in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'increase':\n",
      "[('increase', 0.0), ('decrease', 0.16296813227921048), ('increases', 0.22906224667508612), ('increased', 0.2421957607942684), ('reduction', 0.30917797221922527), ('increasing', 0.3128383733584331), ('decreases', 0.3183826885550045), ('rise', 0.36473526109661814), ('decreasing', 0.37813741818797175), ('decline', 0.38713580180755414)]\n",
      "\n",
      "Words similar to 'exit':\n",
      "[('exit', 0.0), ('exits', 0.3061424059722557), ('exiting', 0.35319962207434497), ('Rockhouse_stumbled_toward', 0.4604811824901355), ('Exit', 0.46161186317846514), ('departure', 0.49619647976589976), ('Exiting', 0.49877576433953585), ('entrance', 0.5159069269023668), ('Inya_Lake_Hotel', 0.5164870685719852), ('exited', 0.5175610373713373)]\n",
      "\n",
      "Words similar to 'simplify':\n",
      "[('simplify', 0.0), ('streamline', 0.2007646530841164), ('simplifying', 0.20776002332833066), ('simplified', 0.2956007112766764), ('Simplifying', 0.3483464454665547), ('simplifies', 0.3527117110292194), ('automate', 0.3580227096056031), ('streamlining', 0.3587974573234537), ('standardize', 0.36239428115319205), ('simplification', 0.38523110607642774)]\n",
      "\n",
      "Words similar to 'clean':\n",
      "[('clean', 0.0), ('Clean', 0.320295038649002), ('mess_Lehmacher', 0.34435368084158413), ('Surprisingly_Zambonis', 0.36185838782706437), ('cleaned', 0.3857115671642216), ('cleaning', 0.3899145916459108), ('cleaner', 0.3940767949093611), ('sober_Boole_added', 0.4339487494533599), ('surgeon_Rochelle_Dicker', 0.4505251283838524), ('NASDAQ_BLDP_provides', 0.454601461891983)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_words = ['increase', 'exit', 'simplify', 'clean']\n",
    "\n",
    "# NOTE: most_similar() func computes cosine similarity between a simple mean of the \n",
    "# projection weight vectors of the given keys and the vectors for each key in the model.\n",
    "# The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
    "# word2vec implementation.\n",
    "\n",
    "def compute_most_similar(embeddings, term, topn = 10):\n",
    "    word_score_dict = {}\n",
    "    a_vec = embeddings[term]\n",
    "    \n",
    "    for word, vec in embeddings.items():\n",
    "        similarity = cosine(vec, a_vec)\n",
    "        word_score_dict[word] = similarity\n",
    "    \n",
    "    # We sort in ascending order and find the words with least scores because we are using cosine distanc here.\n",
    "    \n",
    "    # The cosine distance between two vectors is calculated by subtracting the cosine similarity between those \n",
    "    # vectors from 1, essentially measuring the angular difference between them where a value closer to 0 \n",
    "    # indicates greater similarity and a value closer to 2 indicates greater dissimilarity\n",
    "\n",
    "    sorted_word_scores = sorted(word_score_dict.items(), key=lambda item: item[1])\n",
    "    most_similar_words = sorted_word_scores[:topn]\n",
    "    \n",
    "    return most_similar_words\n",
    "\n",
    "for word in sample_words:\n",
    "    similar = compute_most_similar(word2vec_word_embeddings_dict, word, topn=10)\n",
    "    print(f\"Words similar to '{word}':\\n{similar}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does this happen?**\n",
    "\n",
    "If we go back to see how embeddings are trained, using approaches like Word2Vec, GloVe, etc, we know that main idea is similar. The embeddings are constructed based on the context around the word. This could fool the model into learning the word and its antonym have the same semantic meaning in the context. \n",
    "\n",
    "For example, let's consider the below two sentences:\n",
    "- 'I love Tom and his family'\n",
    "- 'I hate Tom and his family'\n",
    "\n",
    "In the above two sentences, the only change is the word love <-> hate, everything else around the word remains the same. Hence, this makes the model construct the similar embeddings for a word and its antonym.\n",
    "\n",
    "Some resources to tackle this issue:\n",
    "- [Paper](https://arxiv.org/pdf/2004.12835#:~:text=Modern%20word%20embeddings%2C%20such%20as,antonyms.%20%5B)\n",
    "- [Paper](https://link.springer.com/chapter/10.1007/978-3-319-99501-4_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Analogy Tests\n",
    "\n",
    "Let's try coming up with our own analogies. \n",
    "\n",
    "**1. New Analogy Test 1: capital-currency**\n",
    "\n",
    "Here, we follow a similar approach as Mikolov's analogy dataset. We are trying to see the similarity between the capital of a country and the country's currency. Basically, we are trying to evaluate if the embeddings understand that a capital belongs to a particular country, and will it be able to relate this to the country's currency.\n",
    "\n",
    "Example:\n",
    "\n",
    "Capital of India is Delhi, and currency of India is Rupee. So, we form the question like:\n",
    "\n",
    "('Delhi', 'rupee', 'Abu Dhabi', 'dirhams')\n",
    "\n",
    "\n",
    "**2. New Analogy Test 2: sport-type_of_sport**\n",
    "\n",
    "Here, we are trying to see the similarity between a sport and whether the sport is a team sport or a solo sport. We want to evaluate if the model not only identifies a sport but understands the sport and how it is played. \n",
    "\n",
    "Example: \n",
    "\n",
    "Cricket is a sport, and it is a team game. So, we form the question like: \n",
    "\n",
    "('cricket', 'team', 'swimming', 'solo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "    'capital-currency': [\n",
    "    ('Delhi', 'rupee', 'Tehran', 'riel'),\n",
    "    ('Bangkok', 'baht', 'Luanda', 'kwanza'),\n",
    "    ('Moscow', 'ruble', 'Tokyo', 'yen')\n",
    "],\n",
    "\n",
    "'sport-type_of_sport': [\n",
    "    ('cricket', 'team', 'tennis', 'solo'),\n",
    "    ('football', 'group', 'badminton', 'single'),\n",
    "    ('baseball', 'team', 'pickleball', 'duo'),\n",
    "    ('racing', 'solo', 'athletics', 'solo'),\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Evaluating the Word2Vec Model --------------\n",
      "\n",
      "Accuracy on group capital-currency: 0.0\n",
      "Accuracy on group sport-type_of_sport: 0.0\n",
      "Overall accuracy from both groups: 0.0\n",
      "\n",
      "\n",
      "-------------- Evaluating the GloVe Model ----------------\n",
      "Accuracy on group capital-currency: 0.0\n",
      "Accuracy on group sport-type_of_sport: 0.0\n",
      "Overall accuracy from both groups: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('------------- Evaluating the Word2Vec Model --------------\\n')\n",
    "word2vec_acc = measure_accuracy(word2vec_word_embeddings_dict, questions, rand=False)\n",
    "print(f'Overall accuracy from both groups: {word2vec_acc}\\n\\n')\n",
    "\n",
    "\n",
    "print('-------------- Evaluating the GloVe Model ----------------')\n",
    "glove_acc = measure_accuracy(glove_word_embeddings_dict, questions, rand=False)\n",
    "print(f'Overall accuracy from both groups: {glove_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "Both sets of pre-trained embeddings don't seem to work well on the two analogy tasks above. It shows that the embeddings do not understand the inner meaning of the words is unable to connect one word to another and form a sort of relationship or graphical connection. Like even if it is able to understand a country's capital from an embedding, it is unable to relate it to the country's currency from the country's capital. Basically, it is unable to detect in-direct relationships/connections between words. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
